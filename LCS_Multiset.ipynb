{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SgjhXzwX-YY0tpEfPqD7eQu70MPy9nUX",
      "authorship_tag": "ABX9TyMGSNf7DEMfGIMnenKKfQEa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adityachauhan2344/Adityachauhan2344/blob/main/LCS_Multiset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw6IdtDFDJOo",
        "outputId": "f1ba124d-211a-45c2-e2d7-2fea383a2934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest Common Subsequence: []\n",
            "Multiset Percentage: None\n"
          ]
        }
      ],
      "source": [
        "#seq1 = \"\"\n",
        "#seq2 = \"\"\n",
        "\n",
        "def longest_common_subsequence(seq1, seq2):\n",
        "\n",
        "    m = len(seq1)\n",
        "    n = len(seq2)\n",
        "\n",
        "    # Create a 2D table to store the length of LCS\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    # Fill the table using bottom-up approach\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if seq1[i - 1] == seq2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "\n",
        "    # Backtrack to find the LCS\n",
        "    lcs = []\n",
        "    i, j = m, n\n",
        "    while i > 0 and j > 0:\n",
        "        if seq1[i - 1] == seq2[j - 1]:\n",
        "            lcs.append(seq1[i - 1])\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif dp[i - 1][j] > dp[i][j - 1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "\n",
        "    lcs.reverse()\n",
        "    return lcs\n",
        "\n",
        "def multiset_percentage(seq1, seq2):\n",
        "\n",
        "    lcs = longest_common_subsequence(seq1, seq2)\n",
        "    lcs_size = len(lcs)\n",
        "    total_size = max(len(seq1), len(seq2))\n",
        "\n",
        "    if total_size==0:\n",
        "       return\n",
        "\n",
        "    percentage = (lcs_size / total_size) * 100\n",
        "    return percentage\n",
        "\n",
        "def read_lines_from_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "    return [line.strip() for line in lines]\n",
        "\n",
        "def process_file(file_path):\n",
        "    lines = read_lines_from_file(file_path)\n",
        "    num_lines = len(lines)\n",
        "    i = 0\n",
        "    while i < num_lines:\n",
        "        reference = lines[i]\n",
        "        predicted = lines[i + 1]\n",
        "        # Check for hash line and blank lines\n",
        "        if i + 2 < num_lines and lines[i + 2] == \"#\" and i + 4 < num_lines and lines[i + 3] == \"\" and lines[i + 4] == \"\":\n",
        "            lcs = longest_common_subsequence(reference, predicted)\n",
        "            multiset_score = multiset_percentage(reference, predicted)\n",
        "            print(\"Reference:\", reference)\n",
        "            print(\"Predicted:\", predicted)\n",
        "            print(\"Longest Common Subsequence:\", lcs*100)\n",
        "            print(\"Multiset Percentage:\", multiset_score*100)\n",
        "            print(\"-------------------------------\")\n",
        "        i += 5\n",
        "\n",
        "# Test the function with the file containing reference and predicted sentences\n",
        "file_path = \"/content/result_updated.txt\"  # Replace with the actual file path\n",
        "process_file(file_path)\n",
        "\n",
        "print(\"Longest Common Subsequence:\", longest_common_subsequence(seq1, seq2))\n",
        "print(\"Multiset Percentage:\", multiset_percentage(seq1, seq2))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "    # Read the content of the TXT file\n",
        "\n",
        "\n",
        "\n",
        "def lcs(sentence1, sentence2):\n",
        "    m = len(sentence1)\n",
        "    n = len(sentence2)\n",
        "\n",
        "    # Create a table to store the lengths of LCS of subproblems\n",
        "    dp = [[0] * (n+1) for _ in range(m+1)]\n",
        "\n",
        "    # Fill the table using bottom-up approach\n",
        "    for i in range(1, m+1):\n",
        "        for j in range(1, n+1):\n",
        "            if sentence1[i-1] == sentence2[j-1]:\n",
        "                dp[i][j] = dp[i-1][j-1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "\n",
        "    # Reconstruct the LCS from the table\n",
        "    lcs = []\n",
        "    i, j = m, n\n",
        "    while i > 0 and j > 0:\n",
        "        if sentence1[i-1] == sentence2[j-1]:\n",
        "            lcs = sentence1[i-1] + lcs\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif dp[i-1][j] > dp[i][j-1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "\n",
        "    return lcs\n",
        "\n",
        "def multiset_percentage(sentence1, sentence2):\n",
        "    # Create multisets of words in the sentences using Counters\n",
        "    multiset1 = Counter(sentence1)\n",
        "    multiset2 = Counter(sentence2)\n",
        "\n",
        "    # Calculate the intersection and union of multisets\n",
        "    intersection = sum((multiset1 & multiset2).values())\n",
        "    union = sum((multiset1 | multiset2).values())\n",
        "\n",
        "    # Calculate the multiset percentage\n",
        "    percentage = (intersection / union) * 100\n",
        "\n",
        "    return percentage\n",
        "\n",
        "reference_file_path = \"/content/result.txt\"\n",
        "\n",
        "with open(reference_file_path, 'r') as file:\n",
        "  lines = file.readlines()\n",
        "\n",
        "  # Separate the reference and predicted sentences\n",
        "  sentence1 = []\n",
        "  sentence2 = []\n",
        "  skip_line = False\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "      if skip_line:\n",
        "          skip_line = False\n",
        "          continue\n",
        "      if line == \"#\":\n",
        "          skip_line = True\n",
        "          continue\n",
        "\n",
        "      if line:\n",
        "          if line.startswith(\"target string:\"):\n",
        "              sentence1.append(line.split(\"target string:\")[1].strip())\n",
        "          elif line.startswith(\"predicted string:\"):\n",
        "              sentence2.append(line.split(\"predicted string:\")[1].strip())\n",
        "\n",
        "\n",
        "# Find Longest Common Subsequence\n",
        "lcs_result = lcs(sentence1, sentence2)\n",
        "print(\"Longest Common Subsequence:\", lcs_result)\n",
        "\n",
        "# Find Multiset Percentage\n",
        "percentage_result = multiset_percentage(sentence1, sentence2)\n",
        "print(\"Multiset Percentage:\", percentage_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay__P3x8aWNG",
        "outputId": "8bce6192-cc72-4ce7-e293-aadb4df1cfc5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longest Common Subsequence: []\n",
            "Multiset Percentage: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "def longest_common_subsequence(seq1, seq2):\n",
        "    m, n = len(seq1), len(seq2)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            if seq1[i - 1] == seq2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "\n",
        "    # Backtrack to find the LCS\n",
        "    lcs = []\n",
        "    i, j = m, n\n",
        "    while i > 0 and j > 0:\n",
        "        if seq1[i - 1] == seq2[j - 1]:\n",
        "            lcs.append(seq1[i - 1])\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif dp[i - 1][j] > dp[i][j - 1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "\n",
        "    lcs.reverse()\n",
        "    return lcs\n",
        "\n",
        "def multiset_cosine_similarity(set1, set2):\n",
        "    # Calculate the dot product between the two multisets\n",
        "    dot_product = sum((set1 & set2).values())\n",
        "\n",
        "    # Calculate the magnitudes of the two multisets\n",
        "    magnitude_set1 = math.sqrt(sum((count ** 2 for count in set1.values())))\n",
        "    magnitude_set2 = math.sqrt(sum((count ** 2 for count in set2.values())))\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    if magnitude_set1 > 0 and magnitude_set2 > 0:\n",
        "        cosine_similarity = dot_product / (magnitude_set1 * magnitude_set2)\n",
        "    else:\n",
        "        cosine_similarity = 0.0\n",
        "\n",
        "    return cosine_similarity\n",
        "\n",
        "#def extract_sentences_from_file(file_path):\n",
        "\n",
        "def extract_sentences_from_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    target_sentences = []\n",
        "    predicted_sentences = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        target_sentence = lines[i].strip()\n",
        "        predicted_sentence = lines[i + 1].strip()\n",
        "\n",
        "        target_sentences.append(target_sentence)\n",
        "        predicted_sentences.append(predicted_sentence)\n",
        "\n",
        "        # Skip the hash line and two blank lines\n",
        "        i += 5\n",
        "\n",
        "    return target_sentences, predicted_sentences\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "        # Separate the reference and predicted sentences\n",
        "        sentence1 = []\n",
        "        sentence2 = []\n",
        "        skip_line = False\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if skip_line:\n",
        "                skip_line = False\n",
        "                continue\n",
        "            if line == \"#\":\n",
        "                skip_line = True\n",
        "                continue\n",
        "\n",
        "            if line:\n",
        "                if line.startswith(\"target string:\"):\n",
        "                    sentence1.append(line.split(\"target string:\")[1].strip())\n",
        "                elif line.startswith(\"predicted string:\"):\n",
        "                    sentence2.append(line.split(\"predicted string:\")[1].strip())'''\n",
        "\n",
        "# Example usage:\n",
        "file_path = '/content/result_updated.txt'\n",
        "target_sentences, predicted_sentences = extract_sentences_from_file(file_path)\n",
        "\n",
        "# Calculate LCS and Multiset Cosine Similarity for each pair of target and predicted sentences\n",
        "\n",
        "multiset_cosine_similarities = []\n",
        "lcs_array = []\n",
        "\n",
        "for target, predicted in zip(target_sentences, predicted_sentences):\n",
        "    lcs = longest_common_subsequence(target, predicted)\n",
        "    lcs_avg = len(lcs)/len(target)\n",
        "    lcs_array.append(lcs_avg)\n",
        "\n",
        "    #print(\"LCS for:\")\n",
        "    #print(\"Average LCS :\", lcs_avg*100)\n",
        "    #print(\"Target:\", target)\n",
        "    #print(\"Predicted:\", predicted)\n",
        "    #print(\"LCS:\", lcs)\n",
        "\n",
        "\n",
        "    multiset1 = Counter(target.split())\n",
        "    multiset2 = Counter(predicted.split())\n",
        "    cosine_similarity = multiset_cosine_similarity(multiset1, multiset2)\n",
        "    multiset_cosine_similarities.append(cosine_similarity)\n",
        "    #print(\"Multiset Cosine Similarity:\", cosine_similarity)\n",
        "\n",
        "    #print(\"\\n\")\n",
        "\n",
        "average_lcs_score = sum(lcs_array)/len(lcs_array)\n",
        "average_cosine_similarity = sum(multiset_cosine_similarities) / len(multiset_cosine_similarities)\n",
        "\n",
        "print(\"Average Multiset Cosine Similarity:\", average_cosine_similarity*100)\n",
        "print(\"Average LCS :\", average_lcs_score*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRaj6CVRhprT",
        "outputId": "c1336119-dcfc-44a3-f69b-8d1cc85e6286"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Multiset Cosine Similarity: 23.29606044239002\n",
            "Average LCS : 49.127907715833395\n"
          ]
        }
      ]
    }
  ]
}