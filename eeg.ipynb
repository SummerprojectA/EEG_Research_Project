{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiVbvNxPrIT0LNU9C/pJGp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adityachauhan2344/Adityachauhan2344/blob/main/eeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "Ne7qwbicHxdm",
        "outputId": "54650e23-7da1-431a-d699-32a76bd95d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1d149c9bef8a>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'12-ajY-RIPBqzIj79ItoXiEFmRT_a_7bS'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Process the data as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '12-ajY-RIPBqzIj79ItoXiEFmRT_a_7bS'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "import pickle\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "import time\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "!pip install -q transformers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = /datasetSentences.txt\n",
        "\n",
        "\n",
        "# Process the data as needed\n",
        "# ...\n",
        "\n",
        "\n",
        "\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig, BartForSequenceClassification, BertTokenizer, BertConfig, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
        "from data import ZuCo_dataset, SST_tenary_dataset\n",
        "from model_sentiment import FineTunePretrainedTwoStep\n",
        "from config import get_config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    # preds: numpy array: N * 3\n",
        "    # labels: numpy array: N\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def flat_accuracy_top_k(preds, labels,k):\n",
        "    topk_preds = []\n",
        "    for pred in preds:\n",
        "        topk = pred.argsort()[-k:][::-1]\n",
        "        topk_preds.append(list(topk))\n",
        "    # print(topk_preds)\n",
        "    topk_preds = list(topk_preds)\n",
        "    right_count = 0\n",
        "    # print(len(labels))\n",
        "    for i in range(len(labels)):\n",
        "        l = labels[i][0]\n",
        "        if l in topk_preds[i]:\n",
        "            right_count+=1\n",
        "    return right_count/len(labels)\n",
        "\n",
        "def train_model_ZuCo(dataloaders, device, model, criterion, optimizer, scheduler, num_epochs=25, checkpoint_path_best = './checkpoints/text_sentiment_classifier/best/test.pt', checkpoint_path_last = './checkpoints/text_sentiment_classifier/last/test.pt'):\n",
        "    # modified from: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100000000000\n",
        "    best_acc = 0.0\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'dev']:\n",
        "            total_accuracy = 0.0\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for input_word_eeg_features, seq_lens, input_masks, input_mask_invert, target_ids, target_mask, sentiment_labels, sent_level_EEG in tqdm(dataloaders[phase]):\n",
        "\n",
        "                # input_word_eeg_features = input_word_eeg_features.to(device).float()\n",
        "                # input_masks = input_masks.to(device)\n",
        "                # input_mask_invert = input_mask_invert.to(device)\n",
        "                target_ids = target_ids.to(device)\n",
        "                target_mask = target_mask.to(device)\n",
        "                sentiment_labels = sentiment_labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                output = model(input_ids = target_ids, attention_mask = target_mask, return_dict = True, labels = sentiment_labels)\n",
        "                logits = output.logits\n",
        "                loss = output.loss\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    # with torch.autograd.detect_anomaly():\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # calculate accuracy\n",
        "                preds_cpu = logits.detach().cpu().numpy()\n",
        "                label_cpu = sentiment_labels.cpu().numpy()\n",
        "\n",
        "                total_accuracy += flat_accuracy(preds_cpu, label_cpu)\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * sent_level_EEG.size()[0] # batch loss\n",
        "                # print('[DEBUG]loss:',loss.item())\n",
        "                # print('#################################')\n",
        "\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = total_accuracy / len(dataloaders[phase])\n",
        "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
        "            print('{} Acc: {:.4f}'.format(phase, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'dev' and (epoch_acc > best_acc):\n",
        "                best_loss = epoch_loss\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                '''save checkpoint'''\n",
        "                torch.save(model.state_dict(), checkpoint_path_best)\n",
        "                print(f'update best on dev checkpoint: {checkpoint_path_best}')\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "    print('Best val acc: {:4f}'.format(best_acc))\n",
        "    torch.save(model.state_dict(), checkpoint_path_last)\n",
        "    print(f'update last checkpoint: {checkpoint_path_last}')\n",
        "\n",
        "    # write to log\n",
        "    with open(output_log_file_name, 'w') as outlog:\n",
        "        outlog.write(f'best val loss: {best_loss}\\n')\n",
        "        outlog.write('Best val acc: {:4f}'.format(best_acc))\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def train_model_SST(dataloaders, device, model, criterion, optimizer, scheduler, num_epochs=25, checkpoint_path_best = './checkpoints/text_sentiment_classifier/best/test.pt', checkpoint_path_last = './checkpoints/text_sentiment_classifier/last/test.pt'):\n",
        "    # modified from: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100000000000\n",
        "    best_acc = 0.0\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'dev']:\n",
        "            total_accuracy = 0.0\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for input_ids,input_masks,sentiment_labels in tqdm(dataloaders[phase]):\n",
        "\n",
        "                input_ids = input_ids.to(device)\n",
        "                input_masks = input_masks.to(device)\n",
        "                sentiment_labels = sentiment_labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                output = model(input_ids = input_ids, attention_mask = input_masks, return_dict = True, labels = sentiment_labels)\n",
        "                logits = output.logits\n",
        "                loss = output.loss\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                if phase == 'train':\n",
        "                    # with torch.autograd.detect_anomaly():\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # calculate accuracy\n",
        "                preds_cpu = logits.detach().cpu().numpy()\n",
        "                label_cpu = sentiment_labels.cpu().numpy()\n",
        "\n",
        "                total_accuracy += flat_accuracy(preds_cpu, label_cpu)\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * input_ids.size()[0] # batch loss\n",
        "                # print('[DEBUG]loss:',loss.item())\n",
        "                # print('#################################')\n",
        "\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = total_accuracy / len(dataloaders[phase])\n",
        "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
        "            print('{} Acc: {:.4f}'.format(phase, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'dev' and (epoch_acc > best_acc):\n",
        "                best_loss = epoch_loss\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                '''save checkpoint'''\n",
        "                torch.save(model.state_dict(), checkpoint_path_best)\n",
        "                print(f'update best on dev checkpoint: {checkpoint_path_best}')\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "    print('Best val acc: {:4f}'.format(best_acc))\n",
        "    torch.save(model.state_dict(), checkpoint_path_last)\n",
        "    print(f'update last checkpoint: {checkpoint_path_last}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_config('train_sentiment_textbased')\n",
        "\n",
        "    ''' config param'''\n",
        "\n",
        "    num_epoch = args['num_epoch']\n",
        "    # lr = 1e-3 # Bert, RoBerta\n",
        "    # lr = 1e-4 # Bart\n",
        "    lr = args['learning_rate']\n",
        "\n",
        "    dataset_name = args['dataset_name'] # zero-shot setting: using external dataset from stanford sentiment treebank, pass in 'SST'; or pass in 'ZuCo' to train on ZuCo's text-sentiment pairs\n",
        "\n",
        "    dataset_setting = 'unique_sent'\n",
        "\n",
        "    batch_size = args['batch_size']\n",
        "\n",
        "    # model_name = 'pretrain_Bert'\n",
        "    # model_name = 'pretrain_RoBerta'\n",
        "    # model_name = 'pretrain_Bart'\n",
        "    model_name = args['model_name']\n",
        "    print(f'[INFO]model name: {model_name}')\n",
        "\n",
        "    save_path = args['save_path']\n",
        "\n",
        "    if dataset_name == 'ZuCo':\n",
        "        # subject_choice = 'ALL\n",
        "        subject_choice = args['subjects']\n",
        "        print(f'![Debug]using {subject_choice}')\n",
        "        # eeg_type_choice = 'GD\n",
        "        eeg_type_choice = args['eeg_type']\n",
        "        print(f'[INFO]eeg type {eeg_type_choice}')\n",
        "        # bands_choice = ['_t1']\n",
        "        # bands_choice = ['_t1','_t2','_a1','_a2','_b1','_b2','_g1','_g2']\n",
        "        bands_choice = args['eeg_bands']\n",
        "        print(f'[INFO]using bands {bands_choice}')\n",
        "        save_name = f'Textbased_ZuCo_{model_name}_b{batch_size}_{num_epoch}_{lr}_{dataset_setting}_{eeg_type_choice}'\n",
        "    elif dataset_name == 'SST':\n",
        "        save_name = f'Textbased_StanfordSentitmentTreeband_{model_name}_b{batch_size}_{num_epoch}_{lr}'\n",
        "\n",
        "    output_checkpoint_name_best = save_path + f'/best/{save_name}.pt'\n",
        "    output_checkpoint_name_last = save_path + f'/last/{save_name}.pt'\n",
        "\n",
        "\n",
        "    ''' set random seeds '''\n",
        "    seed_val = 312\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    ''' set up device '''\n",
        "    # use cuda\n",
        "    if torch.cuda.is_available():\n",
        "        dev = args['cuda']\n",
        "    else:\n",
        "        dev = \"cpu\"\n",
        "    # CUDA_VISIBLE_DEVICES=0,1,2,3\n",
        "    device = torch.device(dev)\n",
        "    print(f'[INFO]using device {dev}')\n",
        "\n",
        "\n",
        "    ''' load pickle '''\n",
        "    if dataset_name == 'ZuCo':\n",
        "        whole_dataset_dict = []\n",
        "        dataset_path_task1 = './dataset/ZuCo/task1-SR/pickle/task1-SR-dataset.pickle'\n",
        "        with open(dataset_path_task1, 'rb') as handle:\n",
        "            whole_dataset_dict.append(pickle.load(handle))\n",
        "\n",
        "    '''tokenizer'''\n",
        "    if model_name == 'pretrain_Bert':\n",
        "        print('[INFO]pretrained checkpoint: bert-base-cased')\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    elif model_name == 'pretrain_RoBerta':\n",
        "        print('[INFO]pretrained checkpoint: roberta-base')\n",
        "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    elif model_name == 'pretrain_Bart':\n",
        "        print('[INFO]pretrained checkpoint: bart-large')\n",
        "        tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "\n",
        "    ''' set up dataloader '''\n",
        "    if dataset_name == 'ZuCo':\n",
        "        # train dataset\n",
        "        train_set = ZuCo_dataset(whole_dataset_dict, 'train', tokenizer, subject = subject_choice, eeg_type = eeg_type_choice, bands = bands_choice, setting = dataset_setting)\n",
        "        # dev dataset\n",
        "        dev_set = ZuCo_dataset(whole_dataset_dict, 'dev', tokenizer, subject = subject_choice, eeg_type = eeg_type_choice, bands = bands_choice, setting = dataset_setting)\n",
        "\n",
        "    elif dataset_name == 'SST':\n",
        "        SST_SENTIMENT_LABELS = json.load(open('./dataset/stanfordsentiment/ternary_dataset.json'))\n",
        "\n",
        "        SST_dataset = SST_tenary_dataset(SST_SENTIMENT_LABELS, tokenizer)\n",
        "\n",
        "        train_size = int(0.9 * len(SST_dataset))\n",
        "        val_size = len(SST_dataset) - train_size\n",
        "\n",
        "        train_set, dev_set = random_split(SST_dataset, [train_size, val_size])\n",
        "        print('{:>5,} training samples'.format(len(train_set)))\n",
        "        print('{:>5,} validation samples'.format(len(dev_set)))\n",
        "\n",
        "\n",
        "    dataset_sizes = {'train': len(train_set), 'dev': len(dev_set)}\n",
        "    print('[INFO]train_set size: ', len(train_set))\n",
        "    print('[INFO]dev_set size: ', len(dev_set))\n",
        "\n",
        "    # train dataloader\n",
        "    train_dataloader = DataLoader(train_set, batch_size = batch_size, shuffle=True, num_workers=4)\n",
        "    # dev dataloader\n",
        "    val_dataloader = DataLoader(dev_set, batch_size = 1, shuffle=False, num_workers=4)\n",
        "    # dataloaders\n",
        "    dataloaders = {'train':train_dataloader, 'dev':val_dataloader}\n",
        "\n",
        "    ''' set up model '''\n",
        "    if model_name == 'pretrain_Bert':\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-cased',num_labels=3)\n",
        "    elif model_name == 'pretrain_RoBerta':\n",
        "        model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
        "    elif model_name == 'pretrain_Bart':\n",
        "        model = BartForSequenceClassification.from_pretrained('facebook/bart-large', num_labels = 3)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    \"\"\"save config\"\"\"\n",
        "    with open(f'./config/text_sentiment_classifier/{save_name}.json', 'w') as out_config:\n",
        "        json.dump(args, out_config, indent = 4)\n",
        "\n",
        "\n",
        "    ''' training loop '''\n",
        "    ######################################################\n",
        "    '''step one trainig: freeze most of BART params'''\n",
        "    ######################################################\n",
        "\n",
        "    ''' set up optimizer and scheduler'''\n",
        "    optimizer_step1 = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    exp_lr_scheduler_step1 = lr_scheduler.StepLR(optimizer_step1, step_size=10, gamma=0.1)\n",
        "\n",
        "    # TODO: rethink about the loss function\n",
        "    ''' set up loss function '''\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # return best loss model from step1 training\n",
        "    print(f'=== start training {dataset_name} ... ===')\n",
        "    if dataset_name == 'ZuCo':\n",
        "        model = train_model_ZuCo(dataloaders, device, model, criterion, optimizer_step1, exp_lr_scheduler_step1, num_epochs=num_epoch, checkpoint_path_best = output_checkpoint_name_best, checkpoint_path_last = output_checkpoint_name_last)\n",
        "    elif dataset_name == 'SST':\n",
        "        model = train_model_SST(dataloaders, device, model, criterion, optimizer_step1, exp_lr_scheduler_step1, num_epochs=num_epoch, checkpoint_path_best = output_checkpoint_name_best, checkpoint_path_last = output_checkpoint_name_last)\n"
      ],
      "metadata": {
        "id": "GIFYmdIgJdhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sIwWyMxmJeDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hIp_2OnhJeGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lH_KX8W9JeI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2BiBJXOvJeMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}