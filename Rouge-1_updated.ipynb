{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1rEj2-NDxznqr-Fzx3jbyvCHH5Vxudbs2",
      "authorship_tag": "ABX9TyMoFNekKtr6wnF6FfK6wjxi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adityachauhan2344/Adityachauhan2344/blob/main/Rouge-1_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgjtLoLW74xC",
        "outputId": "01454a7d-ddf9-4027-f294-f472a99cc33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU-1 Score: 67.55314004351953\n",
            "Average BLEU-2 Score: 53.273009544293004\n",
            "Average BLEU-3 Score: 42.68297368483343\n",
            "Average BLEU-4 Score: 35.45333475145549\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def calculate_bleu_n_with_smoothing(reference_file_path, n):\n",
        "    # Read the content of the TXT file\n",
        "    with open(reference_file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Separate the reference and predicted sentences\n",
        "    reference_sentences = []\n",
        "    predicted_sentences = []\n",
        "    skip_line = False\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if skip_line:\n",
        "            skip_line = False\n",
        "            continue\n",
        "        if line == \"#\":\n",
        "            skip_line = True\n",
        "            continue\n",
        "\n",
        "        if line:\n",
        "            if line.startswith(\"target string:\"):\n",
        "                reference_sentences.append(line.split(\"target string:\")[1].strip())\n",
        "            elif line.startswith(\"predicted string:\"):\n",
        "                predicted_sentences.append(line.split(\"predicted string:\")[1].strip())\n",
        "\n",
        "    # Create a smoothing function\n",
        "    smoothing = SmoothingFunction().method1\n",
        "\n",
        "    # Calculate BLEU-n scores with smoothing for each sentence\n",
        "    bleu_n_scores = [sentence_bleu([ref_tokens], pred_tokens, weights=(1/n, ) * n, smoothing_function=smoothing)\n",
        "                     for ref_tokens, pred_tokens in zip(reference_sentences, predicted_sentences)]\n",
        "\n",
        "    # Calculate the average BLEU-n score across all sentences\n",
        "    average_bleu_n_score = sum(bleu_n_scores) / len(bleu_n_scores)\n",
        "\n",
        "    return average_bleu_n_score\n",
        "\n",
        "# Example usage with file path\n",
        "reference_file_path = '/content/result.txt'\n",
        "\n",
        "# Calculate average BLEU-1 score with smoothing\n",
        "average_bleu_1_score = calculate_bleu_n_with_smoothing(reference_file_path, 1)\n",
        "\n",
        "# Calculate average BLEU-2 score with smoothing\n",
        "average_bleu_2_score = calculate_bleu_n_with_smoothing(reference_file_path, 2)\n",
        "\n",
        "# Calculate average BLEU-3 score with smoothing\n",
        "average_bleu_3_score = calculate_bleu_n_with_smoothing(reference_file_path, 3)\n",
        "\n",
        "# Calculate average BLEU-4 score with smoothing\n",
        "average_bleu_4_score = calculate_bleu_n_with_smoothing(reference_file_path, 4)\n",
        "\n",
        "print(\"Average BLEU-1 Score:\", average_bleu_1_score*100)\n",
        "print(\"Average BLEU-2 Score:\", average_bleu_2_score*100)\n",
        "print(\"Average BLEU-3 Score:\", average_bleu_3_score*100)\n",
        "print(\"Average BLEU-4 Score:\", average_bleu_4_score*100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "from rouge import Rouge\n",
        "\n",
        "def calculate_rouge_scores(reference_file_path):\n",
        "    # Read the content of the TXT file\n",
        "    with open(reference_file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Separate the reference and predicted sentences\n",
        "    reference_sentences = []\n",
        "    predicted_sentences = []\n",
        "    skip_line = False\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if skip_line:\n",
        "            skip_line = False\n",
        "            continue\n",
        "        if line == \"#\":\n",
        "            skip_line = True\n",
        "            continue\n",
        "\n",
        "        if line:\n",
        "            if line.startswith(\"target string:\"):\n",
        "                reference_sentences.append(line.split(\"target string:\")[1].strip())\n",
        "            elif line.startswith(\"predicted string:\"):\n",
        "                predicted_sentences.append(line.split(\"predicted string:\")[1].strip())\n",
        "\n",
        "    # Initialize the Rouge object\n",
        "    rouge = Rouge()\n",
        "\n",
        "    # Calculate ROUGE scores for each reference and predicted sentence pair\n",
        "    rouge_scores = [rouge.get_scores(predicted, reference)[0] for reference, predicted in zip(reference_sentences, predicted_sentences)]\n",
        "\n",
        "    # Calculate average ROUGE precision, recall, and F1-score across all pairs\n",
        "    average_precision = sum(score['rouge-1']['p'] for score in rouge_scores) / len(rouge_scores)\n",
        "    average_recall = sum(score['rouge-1']['r'] for score in rouge_scores) / len(rouge_scores)\n",
        "    average_f1_score = sum(score['rouge-1']['f'] for score in rouge_scores) / len(rouge_scores)\n",
        "\n",
        "    return average_precision, average_recall, average_f1_score\n",
        "\n",
        "# Example usage with file path\n",
        "reference_file_path = '/content/result.txt'\n",
        "\n",
        "# Calculate average ROUGE precision, recall, and F1-score\n",
        "average_precision, average_recall, average_f1_score = calculate_rouge_scores(reference_file_path)\n",
        "\n",
        "print(\"Average ROUGE Precision:\", average_precision*100)\n",
        "print(\"Average ROUGE Recall:\", average_recall*100)\n",
        "print(\"Average ROUGE F1-Score:\", average_f1_score*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7jnVc9SDKsy",
        "outputId": "d59003d4-94e5-4249-d208-8cebe172b635"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Average ROUGE Precision: 33.7650030670823\n",
            "Average ROUGE Recall: 27.436918711150383\n",
            "Average ROUGE F1-Score: 30.174872994573366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiKWl0VbDLCY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}